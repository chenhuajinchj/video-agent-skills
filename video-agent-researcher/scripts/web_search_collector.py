#!/usr/bin/env python3
"""
ç½‘é¡µç´ ææœé›†å·¥å…·ã€‚
æ”¯æŒæ‰‹åŠ¨ URL åˆ—è¡¨æ¨¡å¼ï¼ŒæŠ“å–ç½‘é¡µæ­£æ–‡å¹¶ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ã€‚
"""

import json
import re
import sys
import urllib.request
import urllib.error
from pathlib import Path
from typing import List, Dict
from datetime import datetime


def clean_html(html: str) -> str:
    """ç®€å•çš„ HTML æ ‡ç­¾æ¸…ç†ï¼Œæå–æ­£æ–‡æ–‡æœ¬ã€‚"""
    # å»é™¤ script å’Œ style æ ‡ç­¾åŠå†…å®¹
    html = re.sub(r"<script[^>]*>.*?</script>", "", html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r"<style[^>]*>.*?</style>", "", html, flags=re.DOTALL | re.IGNORECASE)
    # å»é™¤æ‰€æœ‰ HTML æ ‡ç­¾
    text = re.sub(r"<[^>]+>", "", html)
    # è§£ç å¸¸è§ HTML å®ä½“
    text = text.replace("&nbsp;", " ").replace("&amp;", "&")
    text = text.replace("&lt;", "<").replace("&gt;", ">")
    text = text.replace("&quot;", '"').replace("&#39;", "'")
    # æ¸…ç†å¤šä½™ç©ºç™½
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def fetch_page(url: str, timeout: int = 15) -> str:
    """æŠ“å–ç½‘é¡µå†…å®¹å¹¶æå–æ­£æ–‡ã€‚"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        charset = resp.headers.get_content_charset() or "utf-8"
        html = resp.read().decode(charset, errors="replace")
    return clean_html(html)


def extract_title(text: str) -> str:
    """ä»æ­£æ–‡å‰å‡ è¡Œæå–æ ‡é¢˜ã€‚"""
    lines = [l.strip() for l in text.split("\n") if l.strip()]
    if lines:
        title = lines[0][:80]
        return re.sub(r'[\\/:*?"<>|]', "_", title)
    return "untitled"


def collect_from_urls(urls: List[str], output_dir: str, topic: str) -> List[Dict]:
    """ä» URL åˆ—è¡¨æŠ“å–æ–‡ç« å¹¶ä¿å­˜ã€‚"""
    output_path = Path(output_dir) / "articles"
    output_path.mkdir(parents=True, exist_ok=True)

    results = []
    for i, url in enumerate(urls, 1):
        url = url.strip()
        if not url:
            continue
        print(f"ğŸ” [{i}/{len(urls)}] æ­£åœ¨æŠ“å–: {url}")
        try:
            text = fetch_page(url)
            if len(text) < 100:
                print(f"   âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡")
                continue

            title = extract_title(text)
            filename = f"article_{i:03d}_{title[:40]}.txt"
            filepath = output_path / filename
            filepath.write_text(f"æ¥æº: {url}\næŠ“å–æ—¶é—´: {datetime.now().isoformat()}\n\n{text}", encoding="utf-8")

            results.append({
                "id": f"src_{i:03d}",
                "type": "article",
                "title": title,
                "url": url,
                "file": str(filepath),
                "char_count": len(text)
            })
            print(f"   âœ… å·²ä¿å­˜: {filename} ({len(text)} å­—)")

        except (urllib.error.URLError, urllib.error.HTTPError, TimeoutError) as e:
            print(f"   âŒ æŠ“å–å¤±è´¥: {e}")
            continue

    return results


def load_url_list(topic: str) -> List[str]:
    """æç¤ºç”¨æˆ·è¾“å…¥ URL åˆ—è¡¨ï¼ˆæ‰‹åŠ¨æ¨¡å¼ï¼‰ã€‚"""
    print(f"\nğŸ“‹ ä¸»é¢˜: {topic}")
    print("è¯·è¾“å…¥è¦æŠ“å–çš„ URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰ï¼š")
    urls = []
    while True:
        try:
            line = input().strip()
            if not line:
                break
            if line.startswith("http"):
                urls.append(line)
        except EOFError:
            break
    return urls


def main():
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python web_search_collector.py <topic> <output_dir> [url_file]")
        print("ç¤ºä¾‹: python web_search_collector.py 'è®¤çŸ¥åå·®' ./materials urls.txt")
        print("\nå¦‚æœæä¾› url_fileï¼Œä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
        print("å¦‚æœä¸æä¾› url_fileï¼Œè¿›å…¥äº¤äº’æ¨¡å¼æ‰‹åŠ¨è¾“å…¥ URL")
        sys.exit(1)

    topic = sys.argv[1]
    output_dir = sys.argv[2]

    # ä»æ–‡ä»¶æˆ–äº¤äº’æ¨¡å¼è·å– URL åˆ—è¡¨
    if len(sys.argv) > 3:
        url_file = Path(sys.argv[3])
        if not url_file.exists():
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° URL æ–‡ä»¶: {url_file}")
            sys.exit(1)
        urls = [l.strip() for l in url_file.read_text(encoding="utf-8").splitlines() if l.strip().startswith("http")]
        print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½äº† {len(urls)} ä¸ª URL")
    else:
        urls = load_url_list(topic)

    if not urls:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ URLï¼Œé€€å‡º")
        sys.exit(1)

    print(f"\nğŸš€ å¼€å§‹æŠ“å– {len(urls)} ä¸ªé¡µé¢...\n")
    results = collect_from_urls(urls, output_dir, topic)

    # ä¿å­˜æœç´¢ç»“æœæ‘˜è¦
    summary = {
        "topic": topic,
        "search_date": datetime.now().strftime("%Y-%m-%d"),
        "total_urls": len(urls),
        "successful": len(results),
        "articles": results
    }
    summary_path = Path(output_dir) / "search_results.json"
    summary_path.parent.mkdir(parents=True, exist_ok=True)
    summary_path.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"\nğŸ“Š æœé›†å®Œæˆ:")
    print(f"   æˆåŠŸ: {len(results)}/{len(urls)}")
    print(f"   ç»“æœ: {summary_path}")


if __name__ == "__main__":
    main()
